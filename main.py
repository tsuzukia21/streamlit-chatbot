import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from st_txt_copybutton import txt_copy
import math
from typing import Tuple, Union, List, Dict, Any, Optional
import base64
from st_chat_input_multimodal import multimodal_chat_input

st.set_page_config(layout="wide", page_title="chat bot",page_icon=":material/chat:")

# ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ä¸€å…ƒç®¡ç†
MODEL_CONFIG = {
    "gpt-4.1-nano": {
        "provider": "openai",
        "index": 0,
        "llm_factory": lambda temp: ChatOpenAI(model="gpt-4.1-nano", temperature=temp)
    },
    "claude-sonnet-4": {
        "provider": "anthropic",
        "index": 1,
        "llm_factory": lambda temp: ChatAnthropic(
            temperature=temp,
            model_name="claude-sonnet-4-20250514",
            max_tokens=4096,
            timeout=120,
            max_retries=3
        )
    },
    "gemini-2.5-pro": {
        "provider": "google",
        "index": 2,
        "llm_factory": lambda temp: ChatGoogleGenerativeAI(
            model="gemini-2.5-pro",
            temperature=temp
        )
    }
}

def initialize_session_state():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–ã‚’ä¸€å…ƒç®¡ç†"""
    defaults = {
        "done": True,
        "Clear": False,
        "save": False,
        "stop": False,
        "edit_states": {},
        "total_tokens": 0,
        "system_prompt": "You are an excellent AI assistant.",
        "temperature": 0.7,
        "error_message": "",
        "model_index": 1,
        "chat_history": [],
        "model": "gpt-4.1-nano",
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value
    
    # LLMåˆæœŸåŒ–ï¼ˆmodelãŒè¨­å®šã•ã‚ŒãŸå¾Œï¼‰
    if "llm" not in st.session_state:
        model_name = st.session_state.model
        config = MODEL_CONFIG[model_name]
        st.session_state.llm = config["llm_factory"](st.session_state.temperature)

def get_current_provider() -> str:
    """ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚’å–å¾—"""
    model = st.session_state.get("model", "gpt-4.1-nano")
    return MODEL_CONFIG.get(model, MODEL_CONFIG["gpt-4.1-nano"])["provider"]

def copy_button(text: str, key_suffix: Union[int, str]) -> None:
    copy_button = txt_copy(label="copy", text_to_copy=text.replace("\\n", "\n"), key=f"text_clipboard_chat_{key_suffix}")
    if copy_button:
        st.toast("Copied!")

def check_token() -> bool:
    token_limit = 50000
    message_limit = 30
    def limit_error(msg: str) -> bool:
        st.error(msg, icon="ğŸš¨")
        st.session_state.Clear = True
        st.session_state.done = True
        st.session_state.save = False
        return False
    
    if st.session_state.total_tokens > token_limit:
        persent=min(100, math.floor(100 * st.session_state.total_tokens / token_limit))
        return limit_error(f'Error: Text volume is {persent}% of the limit.  \nPlease delete unnecessary parts or reset the conversation')
    if len(st.session_state.chat_history) > message_limit:
        return limit_error('Error: Conversation limit exceeded. Please reset the conversation')
    return True

def clear_chat():
    st.session_state.chat_history = []
    st.session_state.Clear = False
    st.session_state.total_tokens = 0
    st.session_state.done = True
    st.session_state.error_message = ""
    st.session_state.edit_states = {}
    st.rerun()

def update_system_prompt():
    st.session_state.system_prompt = st.session_state.new_system_prompt

def update_temperature():
    st.session_state.temperature = st.session_state.new_temperature

def update_model():
    """ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆæ™‚ã«LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°"""
    model_name = st.session_state.model
    config = MODEL_CONFIG.get(model_name)
    if config:
        st.session_state.llm = config["llm_factory"](st.session_state.temperature)
        st.session_state.model_index = config["index"]

def on_stop() -> None:
    """stopæŠ¼ä¸‹æ™‚ã«åœæ­¢ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹ã€‚streamlitã®ä»•æ§˜ä¸Šãƒ«ãƒ¼ãƒ—ä¸­æ–­å¾Œã®å‡¦ç†ã¯å®Ÿè¡Œã•ã‚Œãªã„ã®ã§stateã€chat_historyæ›´æ–°ã™ã‚‹ã€‚"""
    response = st.session_state.get("response", "")
    if response:
        st.session_state.chat_history.append(("assistant", response))
    st.session_state.stop = True
    st.session_state.done = True
    st.session_state.Clear = True
    st.session_state.save = False

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–
initialize_session_state()

with st.sidebar.container():
    st.selectbox("model",
                 ("gpt-4.1-nano","claude-sonnet-4","gemini-2.5-pro"),
                 help="You can select the model.",index=st.session_state.model_index,key="model",on_change=update_model)
    st.text_area("system prompt",value=st.session_state.system_prompt,on_change=update_system_prompt,key="new_system_prompt",
                                 help="You can provide a prompt to the system. This is only effective at the first message transmission.")
    st.slider(label="temperature",min_value=0.0, max_value=1.0,on_change=update_temperature,key="new_temperature",
                            value=st.session_state.temperature,help="Controls the randomness of the generated text.")
    
def modify_message(messages, i):
    del messages[i:]
    return messages

def render_markdown(text: str) -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’Markdownè¡¨ç¤ºç”¨ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã™ã‚‹ã€‚
    æ”¹è¡Œã‚’<br>ã«å¤‰æ›ã—ã€ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã™ã‚‹ã€‚
    """
    return text.replace("\n", "<br>").replace("$", "\\$").replace("#", "\\#").replace("_", "\\_")

def render_uploaded_images(files: List[Dict]) -> None:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    """
    for file in files:
        if file.get("type", "").startswith("image/"):
            try:
                base64_data = file['data'].split(',')[1] if ',' in file['data'] else file['data']
                image_bytes = base64.b64decode(base64_data)
                st.image(image_bytes, caption=file['name'], width=200)
            except (ValueError, base64.binascii.Error, KeyError) as e:
                st.warning(f"ç”»åƒ '{file.get('name', 'unknown')}' ã®è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                st.write(f"ğŸ“ {file.get('name', 'unknown')}")

def build_prompt_template(image_urls: Optional[List[str]] = None) -> ChatPromptTemplate:
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    image_urls ãŒ None ã¾ãŸã¯ç©ºã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã€ãã‚Œä»¥å¤–ã¯ç”»åƒä»˜ããƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è¿”ã™ã€‚
    """
    if image_urls:
        # ç”»åƒä»˜ãã®å ´åˆ
        human_content: List[Dict[str, Any]] = [{"type": "text", "text": "{input}"}]
        for url in image_urls:
            human_content.append({
                "type": "image_url",
                "image_url": {"url": url}
            })
        return ChatPromptTemplate.from_messages(
            [
                ("system", st.session_state.system_prompt),
                MessagesPlaceholder(variable_name="conversation"),
                ("human", human_content),
            ]
        )
    else:
        # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®å ´åˆ
        return ChatPromptTemplate.from_messages(
            [
                ("system", st.session_state.system_prompt),
                MessagesPlaceholder(variable_name="conversation"),
                ("human", "{input}"),
            ]
        )

def build_chain(prompt_template: ChatPromptTemplate):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚"""
    return (prompt_template | st.session_state.llm).with_config({"run_name": "Chat", "tags": ["Chat"]})

def stream_response(chain, input_text: str, conversation_history: List[Tuple[str, Union[str, List[Dict[str, Any]]]]], provider: str, message_placeholder) -> Tuple[str, int]:
    """
    å…±é€šã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã€‚
    - ãƒãƒ£ãƒ³ã‚¯çµåˆ
    - stop æŠ¼ä¸‹ãƒã‚§ãƒƒã‚¯
    - ãƒˆãƒ¼ã‚¯ãƒ³é›†è¨ˆï¼ˆgoogle ã¯é€æ¬¡ã€ä»–ã¯æœ€çµ‚ãƒãƒ£ãƒ³ã‚¯ï¼‰
    """
    st.session_state.response = ""
    total_tokens: int = 0
    last_chunk: Any = None
    for chunk in chain.stream({"input": input_text, "conversation": conversation_history}):
        last_chunk = chunk
        if not st.session_state.stop:
            st.session_state.response += chunk.content
            message_placeholder.markdown(st.session_state.response.replace("\n", "  \n") + "â–Œ", unsafe_allow_html=True)
        if provider == "google":
            total_tokens += (getattr(chunk, "usage_metadata", {}) or {}).get("total_tokens", 0)
    if provider != "google" and last_chunk is not None:
        try:
            total_tokens = last_chunk.usage_metadata.get("total_tokens", 0)
        except Exception:
            pass
    message_placeholder.markdown(st.session_state.response.replace("\n", "  \n"))
    return st.session_state.response, total_tokens

def run_chat_turn(
    prompt: str, 
    conversation_history: List[Tuple[str, Union[str, List[Dict[str, Any]]]]], 
    image_urls: Optional[List[str]] = None
) -> Tuple[str, int]:
    """
    ãƒãƒ£ãƒƒãƒˆã‚¿ãƒ¼ãƒ³ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œã‚’è¡Œã†å…±é€šé–¢æ•°ã€‚
    
    Args:
        prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        conversation_history: ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
        image_urls: ç”»åƒã®data URI ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰
    
    Returns:
        (response_text, total_tokens): ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    """
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹ç¯‰
    prompt_template = build_prompt_template(image_urls)
    
    # ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
    chain = build_chain(prompt_template)
    
    # ãƒ—ãƒ­ãƒã‚¤ãƒ€å–å¾—
    provider = get_current_provider()
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œã¨UIè¡¨ç¤º
    st.session_state.response = ""
    with st.chat_message("assistant", avatar=":material/psychology:"):
        col1, col2 = st.columns([9, 1])
        with col1:
            message_placeholder = st.empty()
            message_placeholder.markdown("thinking...")
        with col2:
            _pressed = st.button("stop", on_click=on_stop)
            st.session_state.stop = _pressed
        with col1:
            response, tokens = stream_response(
                chain=chain,
                input_text=prompt,
                conversation_history=conversation_history,
                provider=provider,
                message_placeholder=message_placeholder,
            )
    
    return response, tokens

def render_human_message(message: Tuple[str, Union[str, List[Dict[str, Any]]]], index: int, edit: bool) -> None:
    """
    Render user-side messages.
    """
    
    with st.chat_message("human", avatar=":material/mood:"):
        col1, col2 = st.columns([9, 1])
        with col1:
            if isinstance(message[1], list):
                for item in message[1]:
                    if item["type"] == "text":
                        msg_content = item["text"]
                        st.markdown(render_markdown(msg_content), unsafe_allow_html=True)
                    elif item["type"] == "image_url":
                        st.image(item["image_url"]["url"])
            else:
                msg_content = message[1]
                st.markdown(render_markdown(msg_content), unsafe_allow_html=True)
                
        with col2:
            if edit:
                if st.button("edit", key=f"edit_{index}"):
                    st.session_state.edit_states[index] = True
            else:
                st.button("edit", key=f"dummy_{index}")
                
        if edit and st.session_state.edit_states.get(index):
            st.session_state.new_message = st.text_area("ç·¨é›†ã—ãŸã‚‰saveã—ã¦ãã ã•ã„ã€‚", value=msg_content, key=f"new_message_{index}")
            left, right = st.columns([9, 1])
            with right:
                if st.button("save", key=f"save_{index}", type="primary"):
                    st.session_state.edit_states[index] = False
                    modify_message(st.session_state.chat_history, index)
                    st.session_state.save = True

def render_assistant_message(message: Tuple[str, str], index: int, show_copy_button: bool) -> None:
    """
    Render assistant-side messages.
    """
    col1, col2 = st.columns([9, 1])
    with col1:
        with st.chat_message("assistant", avatar=":material/psychology:"):
            st.markdown(message[1].replace("\n","  \n"), unsafe_allow_html=True)
    with col2:
        if show_copy_button and index == len(st.session_state.chat_history) - 1:
            copy_button(st.session_state.response, index)

def show_chat_history(
    messages: List[Tuple[str, Union[str, List[Dict[str, Any]]]]],
    edit: bool,
    error_message: str,
    new_message: Optional[str] = None,
    show_copy_button: bool = True,
) -> None:
    """
    Display the entire chat history and render new messages as needed.
    """
    for i, message in enumerate(messages):
        if message[0] == "human":
            render_human_message(message, i, edit)
        elif message[0] == "assistant":
            render_assistant_message(message, i, show_copy_button)
    if new_message:
        with st.chat_message("user", avatar=":material/mood:"):
            st.markdown(render_markdown(new_message), unsafe_allow_html=True)
    if error_message:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚  \n{st.session_state.error_message}ã€‚  \nãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã™ã‚‹ã‹å†åº¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚",icon=":material/warning:")

st.title("Streamlit Chatbot")

st.write("**You can converse with the selected model. You can pause the conversation midway and edit the conversation history.**")

user_input = multimodal_chat_input(
    placeholder="Send a message",
    enable_voice_input=True,
    voice_recognition_method="openai_whisper",
    voice_language="ja-JP",
    key="chat_input"
)

show_chat_history(messages=st.session_state.chat_history,edit=True, error_message=st.session_state.error_message, show_copy_button=True)

if user_input is not None:
    st.session_state.error_message = ""
    st.session_state.done = False
    ok = check_token()
    if ok:
        # Extract text from multimodal input
        input_text = user_input.get("text", "")
        input_files = user_input.get("files", [])
        
        # Use text content for LangChain input
        llm_input = input_text if input_text else "Image uploaded"
        
        # Display user message with images
        with st.chat_message("human", avatar=":material/mood:"):
            col1, col2 = st.columns([9, 1])
            with col1:
                if input_text:
                    st.markdown(render_markdown(input_text), unsafe_allow_html=True)
                
                # Display images
                render_uploaded_images(input_files)
        
        # Extract image URLs for prompt template
        image_urls: List[str] = []
        for file in input_files:
            if file.get("type", "").startswith("image/"):
                try:
                    base64_data = file['data'].split(',')[1] if ',' in file['data'] else file['data']
                    # Validate base64 data
                    base64.b64decode(base64_data)
                    image_urls.append(file["data"])
                except (ValueError, base64.binascii.Error) as e:
                    st.warning(f"ç”»åƒ '{file.get('name', 'unknown')}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        
        # Add human message to history
        if image_urls:
            human_payload: List[Dict[str, Any]] = []
            if input_text:
                human_payload.append({"type": "text", "text": input_text})
            for url in image_urls:
                human_payload.append({"type": "image_url", "image_url": {"url": url}})
            st.session_state.chat_history.append(("human", human_payload))
        else:
            st.session_state.chat_history.append(("human", input_text))
        
        # Execute chat turn
        st.session_state.total_tokens = 0
        response, tokens = run_chat_turn(
            prompt=llm_input,
            conversation_history=st.session_state.chat_history[:-1],
            image_urls=image_urls if image_urls else None
        )
        st.session_state.total_tokens = tokens
        st.session_state.chat_history.append(("assistant", response))
        
        # Reset state and rerun
        st.session_state.done = True
        st.session_state.Clear = True
        st.session_state.stop = False
        st.rerun()

if st.session_state.save:
    st.session_state.error_message = ""
    st.session_state.done = False
    prompt = st.session_state.new_message
    show_chat_history(messages=st.session_state.chat_history, edit=False, error_message=st.session_state.error_message, new_message=prompt, show_copy_button=False)
    ok = check_token()
    if not ok:
        st.session_state.save = False
    else:
        # Add edited human message to history
        st.session_state.chat_history.append(("human", prompt))
        
        # Execute chat turn (text only, no images for edited messages)
        st.session_state.total_tokens = 0
        response, tokens = run_chat_turn(
            prompt=prompt,
            conversation_history=st.session_state.chat_history[:-1],
            image_urls=None
        )
        st.session_state.total_tokens = tokens
        st.session_state.chat_history.append(("assistant", response))
        
        # Reset state and rerun
        st.session_state.done = True
        st.session_state.Clear = True
        st.session_state.save = False
        st.session_state.stop = False
        st.rerun()

if st.session_state.Clear:
    left, spacer, right = st.columns([1, 3, 1])
    with right:
        button_clear_chat = st.button("clear chat history",type="primary")
        if button_clear_chat:
            clear_chat()
